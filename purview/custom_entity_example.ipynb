{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0fd17b49f022c8532a3ea2211b69cec9ba36fdf32d5828a8356790cd1fe9ff052",
   "display_name": "Python 3.7.6 64-bit ('mlops-demo': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "fd17b49f022c8532a3ea2211b69cec9ba36fdf32d5828a8356790cd1fe9ff052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Azure Purview for AI - Demo\n",
    "\n",
    "## Initial Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dotenv\n",
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from pyapacheatlas.core import PurviewClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# authenticate and instantiate client\n",
    "tenant_id = os.environ[\"TENANT_ID\"]\n",
    "client_id = os.environ[\"SP_ID\"]\n",
    "client_secret = os.environ[\"SP_SECRET\"]\n",
    "\n",
    "auth = ServicePrincipalAuthentication(\n",
    "    tenant_id=tenant_id,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "client = PurviewClient(account_name=\"ml-purview\", authentication=auth)\n"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "## Problem setting\n",
    "\n",
    "The vast majority of enterprise AI/ML use cases begin with raw data being copied into cloud storage. Usually this is the staging area of the enterprise data lake. \n",
    "\n",
    "In  this demo, we use the storage account `topsecretdata` as a stand-in for the enterprise data lake and the staging area is the container `data`. At the beginning of the demo, we assume that the raw data has just been copied there, and the same file will be replaced for future runs. \n",
    "\n",
    "The raw data in this case are two files: `iris-train.csv` which contains a subset of rows from iris dataset, and `iris-score.csv` which contains the remaining rows, but with the `Species` column removed. \n",
    "\n",
    "We can view this by navigating to Browse Assets > Azure Blob Storage > topsecretdata > data. Click on `iris-train.csv` and examine all the details. Click on \"edit\" and add/update the description.\n",
    "\n",
    "Click on lineage. If lineage is not empty:\n",
    "- click on the box marked TrainETL > switch to asset. \n",
    "- Copy the GUID from the URL\n",
    "- Paste it in the cell below and run.\n",
    "- Navigate back to the `iris-train.csv` asset lineage, hit refresh, and the lineage should be empty.\n",
    "\n",
    "![](./.img/train-lineage.png)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deletes the TrainETL entity for the demo\n",
    "\n",
    "guid=\"bc8b1732-df57-4776-ac79-884784928aa2\"\n",
    "client.delete_entity(guid=guid)"
   ]
  },
  {
   "source": [
    "## Lineage with Azure Data Factory\n",
    "\n",
    "Typically, before data analysis, the raw data is transformed and loaded into a relational database. This could be a DataVault 2.0 model in the enterprise data warehouse, or a small database dedicated to data and ML use cases. \n",
    "\n",
    "Go to the [data factory](https://adf.azure.com/en-us/home?factory=%2Fsubscriptions%2Fd50ade7c-2587-4da8-9c63-fc828541722c%2FresourceGroups%2Frgp-show-weu-aml-databricks%2Fproviders%2FMicrosoft.DataFactory%2Ffactories%2Fadf-mlops-demo), select the pipeline `pl_train_etl` and launch a debug run. The run completes in about 15 seconds. \n",
    "\n",
    "Navigate to the `train-iris.csv` asset again, click on lineage, refresh, and the full lineage should be visible as seen below.\n",
    "\n",
    "![](./.img/train-lineage-full.png)\n",
    "\n",
    "This is en extremely powerful concept on azure since we are able to see that the `traindata` database table is loaded from the `train-iris.csv` file, and that changes in the structure of that file would cause the ETL to break, so the TrainETL Copy Activity and the traindata database table wouls need to be modified. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Customized lineage in Azure Purview\n",
    "\n",
    "Purview is based on Apache Atlas which is a free and open source metadata management tool. As a result we can extend the functionality of Purview bu creating new entities.\n",
    "\n",
    "For example, Azure ML Pipelines are not a built-in type in Purview. Below we have some code to create a custom type definition to represent an Azure ML pipeline. We can then instantiate this type to represent specific pipeline that we are deploying. Note that since the ML pipeline inherits from the built-in `Process` type, we can specify and array of input and output `Dataset` elements to represent data flows in and out of the pipeline. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This code is used to create a custom type to represent an Azure ML Pipeline. No need to run this.\n",
    "\n",
    "```python\n",
    "# Define ML Pipelne Process Type\n",
    "\n",
    "from pyapacheatlas.core import EntityTypeDef, AtlasAttributeDef\n",
    "from pyapacheatlas.core.typedef import Cardinality\n",
    "\n",
    "\n",
    "pipeline_name = AtlasAttributeDef(\n",
    "    name=\"pipeline_name\",\n",
    "    displayName=\"Pipeline Name\",\n",
    "    description=\"Name of the Azure ML pipeline\"\n",
    ")\n",
    "\n",
    "pipeline_owner = AtlasAttributeDef(\n",
    "    name=\"pipeline_owner\",\n",
    "    displayName=\"Pipeline Owner\",\n",
    "    description=\"Name of the main developer of the ML pipeline\"\n",
    ")\n",
    "\n",
    "process = EntityTypeDef(\n",
    "    name=\"azureml_pipeline\",\n",
    "    superTypes=[\"Process\"],\n",
    "    attributeDefs = [pipeline_name, pipeline_owner]\n",
    ")\n",
    "\n",
    "process.to_json()\n",
    "\n",
    "client.upload_typedefs(process)\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Navigate to the `iris-score.csv` asset in the asset browser. Verify that the schema does not contain the label.  \n",
    "\n",
    "Navigate to the lineage, copy the GUID from the url and delete the entity by running the cell below, just as we did before for the `TrainETL` Copy Activity. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid=\"427896aa-371d-407c-9eb9-5bb95598cf4e\"\n",
    "\n",
    "client.delete_entity(guid=guid)"
   ]
  },
  {
   "source": [
    "Navigate back to the `iris-score.csv` asset and click on Lineage and refresh. No lineage should be available. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Go to `iris/score_pipeline.py`, modify the `description` keyword near Line 68. Ensuring you are in the `master` branch, commit and push. The CI pipeline step responsible for publishing the pipeline also updates the lineage in Purview."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}